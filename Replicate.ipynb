{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Fetching Twitter Data\n",
    "\n",
    "I'm using Tweepy to get all the needed data.\n",
    "The code works fine on PyCharm, but I'm not sure why it always failed on Python Notebook. Maybe you can try to run it on different tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TweepError",
     "evalue": "[{'code': 215, 'message': 'Bad Authentication data.'}]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTweepError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f485698653aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#Still trying..\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'3010631110'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mfollower_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfollowers_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NOCHlLLVlNES'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mfriends_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfriends_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NOCHlLLVlNES'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Leo/anaconda/lib/python3.5/site-packages/tweepy-3.6.0-py3.5.egg/tweepy/binder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;31m# Set pagination mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Leo/anaconda/lib/python3.5/site-packages/tweepy-3.6.0-py3.5.egg/tweepy/binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    227\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;31m# Parse the response payload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTweepError\u001b[0m: [{'code': 215, 'message': 'Bad Authentication data.'}]"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "#I deleted these keys because I think they're sensitive....\n",
    "consumer_key=\"\"\n",
    "consumer_secret=\"\"\n",
    "access_token=\"\"\n",
    "access_token_secret=\"\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#Trying out whether the tweepy is working fine.....\n",
    "#public_tweets = api.home_timeline()\n",
    "#for tweet in public_tweets:\n",
    " #   print(tweet.text)\n",
    "\n",
    "    \n",
    "#Still trying..\n",
    "user = api.get_user('3010631110')\n",
    "follower_id = api.followers_ids('NOCHlLLVlNES')\n",
    "friends_ids = api.friends_ids('NOCHlLLVlNES')\n",
    "follower = api.followers('NOCHlLLVlNES')\n",
    "#print(friends_ids)\n",
    "#print(follower)\n",
    "print(user)\n",
    "\n",
    "\n",
    "\"\"\"print(user.screen_name)\n",
    "print(user.followers_count)\n",
    "for friend in user.friends():\n",
    "   print(friend.screen_name)\n",
    "   print(friend.id)\n",
    "\n",
    "myself = api.me()\n",
    "print(myself)\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "saveFile = open('Real_Twitter_Scraper.txt','w')\n",
    "\n",
    "class CustomStreamListener(tweepy.StreamListener):\n",
    "    def on_status(self, status):\n",
    "\n",
    "        if '' in status.text.lower():\n",
    "            #In these lines, I may choose to save different things if I need.\n",
    "            #print(status.text)\n",
    "            #print(status.name)\n",
    "            #print(status.screen_name)\n",
    "            print(status.id)\n",
    "            for text in status.text:\n",
    "                if text.startswith('RT'):\n",
    "                    continue\n",
    "                else:\n",
    "                    #saveFile.write(str(id))\n",
    "                    saveFile.write(text)\n",
    "\n",
    "                    #print('Written succeed!')\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        print >> sys.stderr, 'Encountered error with status code:', status_code\n",
    "        return True # Don't kill the stream\n",
    "\n",
    "    def on_timeout(self):\n",
    "        print >> sys.stderr, 'Timeout...'\n",
    "        return True # Don't kill the stream\n",
    "\n",
    "\n",
    "#Following 2 lines will return infinite fetching. So I quoted em.\n",
    "#sapi = tweepy.streaming.Stream(auth, CustomStreamListener())\n",
    "#sapi.filter(track=['Trump'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Cosine Similarity Cluster\n",
    "I used your scluster and modified a little bit. Now, the cluster will return the sorted clusters with the most popular cluster at the beginning. \n",
    "\n",
    "However, there's a FLAW. I didn't remove your original function inside, which means the function will return the original return values first, then run my modified code. I left this \"flaw\" here because of two reasons:\n",
    "    1. I may need to view the results by myself. Modified version is not that readable.\n",
    "    2. I'm lazy...zzzzz...\n",
    "LOL... OK, just kidding. I promise I'll make the code nice and clean during the summer vocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "DocoptExit",
     "evalue": "usage:\n    sclust [--help --threshold <T> --update-norms <N>]",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mDocoptExit\u001b[0m\u001b[0;31m:\u001b[0m usage:\n    sclust [--help --threshold <T> --update-norms <N>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"A command-line tool to quickly cluster sentences.\n",
    "\n",
    "usage:\n",
    "    sclust [--help --threshold <T> --update-norms <N>]\n",
    "\n",
    "Options\n",
    "    -h, --help\n",
    "    -t, --threshold <N>    Similarity threshold in [0,1]. Higher means sentences must be more similar to be merged. [default: .2]\n",
    "    -u, --update-norms <N>    Update cluster norms every N documents. Larger values reduce run-time, but sacrifice accuracy. [default: 1]\n",
    "\"\"\"\n",
    "from collections import Counter, defaultdict\n",
    "from docopt import docopt\n",
    "from math import sqrt, log10\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "def norm(tokens, idfs):\n",
    "    return sqrt(np.sum((count * idfs[token])**2\n",
    "                       for token, count in tokens.items()))\n",
    "\n",
    "def norm_dict(tokens, idfs):\n",
    "    d = defaultdict(lambda: 0)\n",
    "    denom = 0\n",
    "    for token, count in tokens.items():\n",
    "        v = count * idfs[token]\n",
    "        d[token] = v\n",
    "        denom += v*v\n",
    "    denom = sqrt(denom)\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    for token, val in d.items():\n",
    "        d[token] = val / denom\n",
    "    return d\n",
    "\n",
    "def tokenize(line):\n",
    "    return re.findall('\\w+', line.lower())\n",
    "\n",
    "def idf(token, doc_freqs, docnum):\n",
    "    return log10((docnum + 1) / doc_freqs[token])\n",
    "\n",
    "class Cluster:\n",
    "\n",
    "    def __init__(self, token_counts, order):\n",
    "        self.token_counts = Counter(token_counts)\n",
    "        self.cluster_norm = None\n",
    "        self.docnum = 0\n",
    "        self.strings = []\n",
    "        self.doc_freqs = Counter()\n",
    "        self.order = order\n",
    "\n",
    "    def add(self, tokens):\n",
    "        self.token_counts.update(tokens)\n",
    "    \n",
    "    def add_string(self, string, tokens, index):\n",
    "        self.docnum += 1\n",
    "        self.doc_freqs.update(tokens)\n",
    "        self.strings.append((string, tokens, index))\n",
    "        \n",
    "    def calculate_idf(self):\n",
    "        self.idfs = {token: idf(token, self.doc_freqs, self.docnum) for token, value in self.doc_freqs.items()}\n",
    "        \n",
    "    def score(self, normed_doc, idfs, do_update_norm):\n",
    "        if do_update_norm or not self.cluster_norm:\n",
    "            self.normed_cluster = norm_dict(self.token_counts, idfs)\n",
    "        return sum(value * self.normed_cluster[token] for token, value in normed_doc.items())\n",
    "\n",
    "def update_index(index, clusterid, tokens):\n",
    "    for t in tokens:\n",
    "        index[t].add(clusterid)\n",
    "\n",
    "def search_index(index, top_words):\n",
    "    clusters = set()\n",
    "    for w in top_words:\n",
    "        clusters |= index[w]\n",
    "    return clusters\n",
    "\n",
    "def run(threshold, norm_update):\n",
    "    order = 0\n",
    "    doc_freqs = Counter()\n",
    "    clusters = []\n",
    "    docnum = 0\n",
    "    index = defaultdict(set)\n",
    "    strings = []\n",
    "    for line in sys.stdin:\n",
    "        if line == \"end\\n\":\n",
    "            break\n",
    "        strings.append(line.strip())\n",
    "    for line in strings:\n",
    "        docnum += 1        \n",
    "        tokens = Counter(tokenize(line))\n",
    "        if len(tokens) == 0:\n",
    "            continue\n",
    "        doc_freqs.update(tokens)\n",
    "    idfs = {token: idf(token, doc_freqs, docnum) for token, value in doc_freqs.items()}\n",
    "    i = 0\n",
    "    for line in strings:\n",
    "        i += 1\n",
    "        tokens = Counter(tokenize(line))\n",
    "        do_update_norm = True if i % norm_update == 0 else False\n",
    "        normed_doc = norm_dict(tokens, idfs)\n",
    "        # What are the five words with highest tfidf weight? Use to filter comparisons.\n",
    "        top_words = sorted(tokens, key=lambda x: -normed_doc[x])[:5]  # (doc_freqs[token], token) for token in tokens)\n",
    "        best_cluster = -1\n",
    "        best_score = -1\n",
    "        for ci in search_index(index, top_words):\n",
    "            cluster = clusters[ci]\n",
    "            score = cluster.score(normed_doc, idfs, do_update_norm)\n",
    "            if score > best_score and score > threshold:\n",
    "                best_cluster = ci\n",
    "                best_score = score\n",
    "        if best_cluster == -1:\n",
    "            clusters.append(Cluster(tokens, order))\n",
    "            update_index(index, len(clusters)-1, tokens)\n",
    "            print('%d\\t%s\\t-' % (len(clusters)-1, line))\n",
    "            clusters[len(clusters)-1].add_string(line, tokens, i-1)\n",
    "            order += 1\n",
    "        else:\n",
    "            clusters[best_cluster].add(tokens)\n",
    "            update_index(index, best_cluster, tokens)\n",
    "            print('%d\\t%s\\t%g' % (best_cluster, line, best_score))\n",
    "            clusters[best_cluster].add_string(line, tokens, i-1)\n",
    "        sys.stdout.flush()\n",
    "    clusters = sorted(clusters, key = lambda c: len(c.strings), reverse = True)\n",
    "    i = 0\n",
    "    for cluster in clusters:\n",
    "        cluster.calculate_idf()\n",
    "        scores = []\n",
    "        for string, tokens, index in cluster.strings:\n",
    "            normed_doc = norm_dict(tokens, cluster.idfs)\n",
    "            score = cluster.score(normed_doc, cluster.idfs, False)\n",
    "            scores.append((string, score, index))\n",
    "        k = 10\n",
    "        top_k = sorted(scores, key = lambda t: t[1], reverse = True)[:k]\n",
    "        print(\"\\nTop %d strings in cluster %d:\" % (k, cluster.order))\n",
    "        n = 0\n",
    "        for string, score, index in top_k:\n",
    "            print(\"%d\\t%s\\t%g\\tAt %d\" % (n, string, score, index))\n",
    "            n += 1\n",
    "        i += 1\n",
    "    \n",
    "\n",
    "# Weirdness when piping to unix tools. See http://stackoverflow.com/a/26736013/1756896\n",
    "def _void_f(*args,**kwargs):\n",
    "    pass\n",
    "\n",
    "def main():\n",
    "    args = docopt(__doc__)\n",
    "    try:\n",
    "        run(float(args['--threshold']),\n",
    "            int(args['--update-norms']))\n",
    "    except (BrokenPipeError, IOError):\n",
    "        sys.stdout.write = _void_f\n",
    "        sys.stdout.flush = _void_f\n",
    "        sys.exit()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Visualization\n",
    "Now, I'm using NetworkX to draw the relationships of related accounts. By visualization, some interesting relationships will merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdoAAAFBCAYAAADQRW4vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABPpJREFUeJzt1UENACAQwDDAv+dDxUJCWgX7bc/MLAAgcV4HAMDPjBYA\nQkYLACGjBYCQ0QJAyGgBIGS0ABAyWgAIGS0AhIwWAEJGCwAhowWAkNECQMhoASBktAAQMloACBkt\nAISMFgBCRgsAIaMFgJDRAkDIaAEgZLQAEDJaAAgZLQCEjBYAQkYLACGjBYCQ0QJAyGgBIGS0ABAy\nWgAIGS0AhIwWAEJGCwAhowWAkNECQMhoASBktAAQMloACBktAISMFgBCRgsAIaMFgJDRAkDIaAEg\nZLQAEDJaAAgZLQCEjBYAQkYLACGjBYCQ0QJAyGgBIGS0ABAyWgAIGS0AhIwWAEJGCwAhowWAkNEC\nQMhoASBktAAQMloACBktAISMFgBCRgsAIaMFgJDRAkDIaAEgZLQAEDJaAAgZLQCEjBYAQkYLACGj\nBYCQ0QJAyGgBIGS0ABAyWgAIGS0AhIwWAEJGCwAhowWAkNECQMhoASBktAAQMloACBktAISMFgBC\nRgsAIaMFgJDRAkDIaAEgZLQAEDJaAAgZLQCEjBYAQkYLACGjBYCQ0QJAyGgBIGS0ABAyWgAIGS0A\nhIwWAEJGCwAhowWAkNECQMhoASBktAAQMloACBktAISMFgBCRgsAIaMFgJDRAkDIaAEgZLQAEDJa\nAAgZLQCEjBYAQkYLACGjBYCQ0QJAyGgBIGS0ABAyWgAIGS0AhIwWAEJGCwAhowWAkNECQMhoASBk\ntAAQMloACBktAISMFgBCRgsAIaMFgJDRAkDIaAEgZLQAEDJaAAgZLQCEjBYAQkYLACGjBYCQ0QJA\nyGgBIGS0ABAyWgAIGS0AhIwWAEJGCwAhowWAkNECQMhoASBktAAQMloACBktAISMFgBCRgsAIaMF\ngJDRAkDIaAEgZLQAEDJaAAgZLQCEjBYAQkYLACGjBYCQ0QJAyGgBIGS0ABAyWgAIGS0AhIwWAEJG\nCwAhowWAkNECQMhoASBktAAQMloACBktAISMFgBCRgsAIaMFgJDRAkDIaAEgZLQAEDJaAAgZLQCE\njBYAQkYLACGjBYCQ0QJAyGgBIGS0ABAyWgAIGS0AhIwWAEJGCwAhowWAkNECQMhoASBktAAQMloA\nCBktAISMFgBCRgsAIaMFgJDRAkDIaAEgZLQAEDJaAAgZLQCEjBYAQkYLACGjBYCQ0QJAyGgBIGS0\nABAyWgAIGS0AhIwWAEJGCwAhowWAkNECQMhoASBktAAQMloACBktAISMFgBCRgsAIaMFgJDRAkDI\naAEgZLQAEDJaAAgZLQCEjBYAQkYLACGjBYCQ0QJAyGgBIGS0ABAyWgAIGS0AhIwWAEJGCwAhowWA\nkNECQMhoASBktAAQMloACBktAISMFgBCRgsAIaMFgJDRAkDIaAEgZLQAEDJaAAgZLQCEjBYAQkYL\nACGjBYCQ0QJAyGgBIGS0ABAyWgAIGS0AhIwWAEJGCwAhowWAkNECQMhoASBktAAQMloACBktAISM\nFgBCRgsAIaMFgJDRAkDIaAEgZLQAEDJaAAgZLQCEjBYAQkYLACGjBYCQ0QJAyGgBIGS0ABAyWgAI\nGS0AhIwWAEJGCwAhowWAkNECQMhoASBktAAQMloACBktAISMFgBCRgsAIaMFgJDRAkDIaAEgZLQA\nEDJaAAgZLQCEjBYAQkYLACGjBYCQ0QJAyGgBIGS0ABAyWgAIXTLABn5rTILRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1090393c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib as mb\n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline\n",
    "graph = nx.DiGraph()\n",
    "#It's an unfinished version. This function should read the lists in the .txt file.\n",
    "#But I'm just trying to show the thought of structure in my mind.\n",
    "list_ids = []\n",
    "for id in list_ids:\n",
    "    for id2 in list_ids:\n",
    "        if id == id2:\n",
    "            continue\n",
    "        if id.is_friendship(id2):\n",
    "            graph.add_edges_from([(id, id2)])\n",
    "\n",
    "\n",
    "nx.draw(graph, with_labels=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
